{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/linnea-backgard/ai-ml-principles-exercises/blob/main/ML-training/creating-reports-with-WandB/preprocess_with_wandb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a762d7ae",
      "metadata": {
        "id": "a762d7ae"
      },
      "source": [
        "## Install dependencies and import packages\n",
        "First we need to install the libraries we will be using. We will use `numpy` for generic matrix operations and `tensorflow` for deep learning operations such as convolutions, pooling and training (backpropagation)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a038785e",
      "metadata": {
        "id": "a038785e"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install numpy tensorflow wandb\n",
        "\n",
        "import wandb\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "\n",
        "date_and_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "wandb_run = wandb.init(\n",
        "    project=\"ai-ml-exercise\",\n",
        "    name=f\"preprocessing {date_and_time}\"\n",
        ")"
      ],
      "metadata": {
        "id": "14dPZ0SvIeG1"
      },
      "id": "14dPZ0SvIeG1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "084f8685",
      "metadata": {
        "id": "084f8685"
      },
      "source": [
        "## Download the data\n",
        "We use a digit classification dataset called *MNIST*. This code downloads and loads the images together with their true labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48724cbc",
      "metadata": {
        "id": "48724cbc"
      },
      "outputs": [],
      "source": [
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define helper functions for logging histograms to WandB\n",
        "Weights and Biases has built in functions to generate histograms of data. These functions do however upload every sample to the WandB server before creating the histogram. This is useful to create interactive graphs, but if logging data from many samples it is too slow to upload every single sample. Instead we can create a histogram with `numpy` and only upload the bucket edges and the counts. We then create a bar plot in WandB to visualize the count for each bucket."
      ],
      "metadata": {
        "id": "3jhfLSh1Mt2u"
      },
      "id": "3jhfLSh1Mt2u"
    },
    {
      "cell_type": "code",
      "source": [
        "def log_bar(x, y, title, x_name=\"x\", y_name=\"y\", keep_order=False):\n",
        "    if keep_order:\n",
        "        x = [f\"{idx}: {x_}\" for idx, x_ in enumerate(x)] # Make sure alphabetical sorting works\n",
        "    table = wandb.Table(\n",
        "        data=[[x, y] for x, y in zip(x, y)],\n",
        "        columns=[x_name, y_name]\n",
        "    )\n",
        "    wandb.log({title: wandb.plot.bar(table, x_name, y_name, title=title)})\n",
        "\n",
        "\n",
        "def create_histogram(data, min_value=None, max_value=None, bins=10):\n",
        "    if min_value is None:\n",
        "        min_value = data.min()\n",
        "    if max_value is None:\n",
        "        max_value = data.max()\n",
        "\n",
        "    if isinstance(bins, int):\n",
        "        bin_edges = np.linspace(min_value, max_value, num=bins)\n",
        "    else:\n",
        "        bin_edges = bins\n",
        "        \n",
        "    numbers, _ = np.histogram(data, bins=bin_edges)\n",
        "    bin_names = [f\"{lower:.1f}-{upper:.1f}\" for lower, upper in zip(bin_edges[:-1], bin_edges[1:])]\n",
        "\n",
        "    return bin_names, numbers"
      ],
      "metadata": {
        "id": "cvBkQQ2Ejc1e"
      },
      "id": "cvBkQQ2Ejc1e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize the labels\n",
        "Except from knowing what the training images looks like it is interesting to know how many images of each label type we have. This is important since if we are lacking data from some label, we will not be able to train a network to recognize images with that label."
      ],
      "metadata": {
        "id": "FjVrbT9asqMp"
      },
      "id": "FjVrbT9asqMp"
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "num_images_per_label = Counter(y_train)\n",
        "x, num_images = zip(*((str(x_), num_) for x_, num_ in sorted(num_images_per_label.items())))\n",
        "log_bar(x, num_images, \"Labels in training data\", x_name=\"Label\", y_name=\"# images\")\n",
        "\n",
        "num_images_per_label = Counter(y_test)\n",
        "x, num_images = zip(*((str(x_), num_) for x_, num_ in sorted(num_images_per_label.items())))\n",
        "log_bar(x, num_images, \"Labels in test data\", x_name=\"Label\", y_name=\"# images\")"
      ],
      "metadata": {
        "id": "TZpNrm94rcU2"
      },
      "id": "TZpNrm94rcU2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize the training data\n",
        "To get a feeling for what the data looks like we will plot one of the training images and its corresponding label. We will also plot a histogram over all the values in all the images so that we can see how the intensity in the images is distributed."
      ],
      "metadata": {
        "id": "Cz_W0GwYRIAF"
      },
      "id": "Cz_W0GwYRIAF"
    },
    {
      "cell_type": "code",
      "source": [
        "i = 0\n",
        "print(f\"Sample {i} is number {y_train[i]}\")\n",
        "plt.imshow(x_train[0])\n",
        "\n",
        "image = wandb.Image(x_train[0], caption=f\"Training sample {i} is a {y_train[i]}\")\n",
        "wandb.log({\"Example training image\": image})"
      ],
      "metadata": {
        "id": "cIFy0YWLO-a9"
      },
      "id": "cIFy0YWLO-a9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Log the datasets minimum and maximum intensities and datatype to the WandB summary\n",
        "min_value = min(x_train.min(), x_test.min())\n",
        "max_value = max(x_train.max(), x_test.max())\n",
        "wandb_run.summary[\"raw\"] = {\"min\": min_value, \"max\": max_value, \"dtype\": str(x_train.dtype)}\n",
        "\n",
        "# Create a new histogram of the image pixels intensities\n",
        "bin_names, train_hist = create_histogram(x_train)\n",
        "log_bar(bin_names, train_hist, \"Raw training data\", x_name=\"bin\", y_name=\"# pixels\", keep_order=True)"
      ],
      "metadata": {
        "id": "fLZX76QOIcTG"
      },
      "id": "fLZX76QOIcTG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a heatmap over all active pixels from all frames\n",
        "heatmap = np.mean(x_train, axis=0)\n",
        "plt.imshow(heatmap)\n",
        "wandb.log({\"Heatmap of training images\": wandb.Image(np.expand_dims(heatmap, axis=2), caption=\"The mean of all images in the training set\")})\n",
        "\n",
        "# Log a histogram of the average value for each pixel through out the\n",
        "# training dataset. This shows us how many of the pixels that are\n",
        "# always zero in all frames.\n",
        "bin_names, heatmap_hist = create_histogram(heatmap.flatten(), bins=[0, 1, 10, 30, 100, 255])\n",
        "log_bar(bin_names, heatmap_hist, \"Average value per pixel in training data\", x_name=\"bin\", y_name=\"# pixels\", keep_order=True)"
      ],
      "metadata": {
        "id": "v4EKkknJmZ8n"
      },
      "id": "v4EKkknJmZ8n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess the data\n",
        "Now that we understand the data we are ready to preprocess the images to make them suitable for training of a deep learning model."
      ],
      "metadata": {
        "id": "P_5M0D6eSY-S"
      },
      "id": "P_5M0D6eSY-S"
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize the values to the range -1...1\n",
        "x_train_norm = x_train / 128 - 1\n",
        "x_test_norm = x_test / 128 - 1"
      ],
      "metadata": {
        "id": "ekWqPTStOdM2"
      },
      "id": "ekWqPTStOdM2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new histogram of the modified values\n",
        "min_value = min(x_train_norm.min(), x_test_norm.min())\n",
        "max_value = max(x_train_norm.max(), x_test_norm.max())\n",
        "wandb_run.summary[\"preprocessed\"] = {\"min\": min_value, \"max\": max_value, \"dtype\": str(x_train_norm.dtype)}\n",
        "\n",
        "bin_names, train_hist = create_histogram(x_train_norm)\n",
        "log_bar(bin_names, train_hist, \"Preprocessed training data\", x_name=\"bin\", y_name=\"# pixels\", keep_order=True)"
      ],
      "metadata": {
        "id": "FzDuNQreS8H_"
      },
      "id": "FzDuNQreS8H_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i = 0\n",
        "print(f\"Sample {i} is number {y_train[i]}\")\n",
        "plt.imshow(x_train_norm[0])\n",
        "\n",
        "image = wandb.Image(x_train_norm[0], caption=f\"Training sample {i} is a {y_train[i]}\")\n",
        "wandb.log({\"Example training image (preprocessed)\": image})"
      ],
      "metadata": {
        "id": "vkjgqam-Ve9e"
      },
      "id": "vkjgqam-Ve9e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Subsample the test data set\n",
        "If the test data set has different number of samples from different labels we will need to weight the different classes differently when evaluating our model if we want each number to be treated as equally importatant. Another option is to subsample so that the test data set has the same number of each label and ignoring the other images."
      ],
      "metadata": {
        "id": "T7_tZ64Jv-pw"
      },
      "id": "T7_tZ64Jv-pw"
    },
    {
      "cell_type": "code",
      "source": [
        "num_images_per_label = Counter(y_test)\n",
        "min_number_of_labels = min(num_images_per_label.values())\n",
        "\n",
        "indexes_to_keep = []\n",
        "for label in num_images_per_label.keys():\n",
        "  indexes_to_keep.extend(\n",
        "      np.random.choice(\n",
        "          np.where(y_test == label)[0],\n",
        "          size=min_number_of_labels,\n",
        "          replace=False\n",
        "      ).tolist()\n",
        "  )\n",
        "\n",
        "np.random.shuffle(indexes_to_keep)\n",
        "x_test_norm_subsamp = x_test_norm[indexes_to_keep]\n",
        "y_test_subsamp = y_test[indexes_to_keep]"
      ],
      "metadata": {
        "id": "UQeTmDj2YfTF"
      },
      "id": "UQeTmDj2YfTF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test_norm_subsamp.shape"
      ],
      "metadata": {
        "id": "WiDAbwj6zN44"
      },
      "id": "WiDAbwj6zN44",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify that the labels and images are still in sync\n",
        "print(f\"This should be a {y_test_subsamp[0]}\")\n",
        "plt.imshow(x_test_norm_subsamp[0])"
      ],
      "metadata": {
        "id": "Lnfbicbsxuk1"
      },
      "id": "Lnfbicbsxuk1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Log the new distribution\n",
        "num_images_per_label = Counter(y_test_subsamp)\n",
        "labels_, num_images = zip(*((str(label), number) for label, number in sorted(num_images_per_label.items())))\n",
        "log_bar(labels_, num_images, \"Labels in test data (subsampled)\", x_name=\"Label\", y_name=\"# images\")"
      ],
      "metadata": {
        "id": "8D9xkzDmxGqM"
      },
      "id": "8D9xkzDmxGqM",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}